{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "355dc64d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Simple NN in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5435bbe0-95d5-43c9-ac54-a20573b7ded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                                      \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple Neural Network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 10)  # Input layer ‚Üí Hidden layer (10 neurons)\n",
    "        self.relu = nn.ReLU()  # Activation function\n",
    "        self.fc2 = nn.Linear(10, 1)  # Hidden layer ‚Üí Output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x  \n",
    "\n",
    "# Create model instance\n",
    "model = SimpleNN()\n",
    "\n",
    "# Define Loss and Optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print(model)  # Check the network structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf496587-5cfe-48b8-973c-ee10abdf94db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# A network to learn a simple function: y = 2x + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712c4dec-98a6-4475-8e5a-fae429dd2c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate Training Data\n",
    "x_train = torch.tensor([[i] for i in range(10)], dtype=torch.float32)\n",
    "y_train = 2 * x_train + 3  # y = 2x + 3\n",
    "\n",
    "# Define the Neural Network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 10)  # Input Layer to Hidden Layer\n",
    "        self.relu = nn.ReLU()        # Activation Function\n",
    "        self.fc2 = nn.Linear(10, 1)  # Hidden Layer to Output Layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = SimpleNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training the Model\n",
    "epochs = 1000\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    output = model(x_train)  # Forward pass\n",
    "    loss = criterion(output, y_train)  # Compute loss\n",
    "    loss.backward()  # Backpropagation\n",
    "    optimizer.step()  # Update weights\n",
    "    \n",
    "    losses.append(loss.item())  # Store loss for visualization\n",
    "    \n",
    "    if epoch % 200 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
    "\n",
    "# Plot Loss Curve\n",
    "plt.plot(range(epochs), losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.show()\n",
    "\n",
    "# Test the Model\n",
    "x_test = torch.tensor([[10.0]])  # Input: x = 10\n",
    "y_pred = model(x_test).item()\n",
    "print(f'Prediction for x=10: {y_pred:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22df913e-9387-4501-8ef1-ff11904e7fa0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# PyTorch Essentials for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d116b6-8f4f-410d-916b-a641980aff0a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ‚úÖ Core PyTorch Components\n",
    "- **`torch`** ‚Üí Core PyTorch library.\n",
    "- **`torch.nn`** ‚Üí Provides tools to build neural networks.\n",
    "- **`torch.optim`** ‚Üí Optimizers for updating model weights.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Defining the Dataset\n",
    "- **`x_train`** ‚Üí Input values (0 to 9).\n",
    "- **`y_train`** ‚Üí Target values (`y = 2x + 3`).\n",
    "\n",
    "üìå **What‚Äôs happening?**  \n",
    "We're simulating a real-world dataset where `x` is some input (e.g., time, size, temperature), and `y` is the corresponding output (e.g., price, demand, prediction).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Fully Connected Layers (`nn.Linear`)\n",
    "```python\n",
    "self.fc1 = nn.Linear(1, 10)  # 1 input neuron ‚Üí 10 hidden neurons\n",
    "self.fc2 = nn.Linear(10, 1)  # 10 hidden neurons ‚Üí 1 output neuron\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Activation Function (ReLU)\n",
    "```python\n",
    "self.relu = nn.ReLU()  # Introduces non-linearity\n",
    "```\n",
    "\n",
    "üìå **Why ReLU?**\n",
    "- Prevents **vanishing gradients** (a problem in deep networks).\n",
    "- **Faster training** compared to **sigmoid** or **tanh**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Loss Function (Measures Error)\n",
    "- **MSELoss() (Mean Squared Error)** ‚Üí Used for regression problems.\n",
    "\n",
    "üìå **Loss Interpretation**  \n",
    "- **High loss** ‚Üí Model is performing poorly.  \n",
    "- **Low loss** ‚Üí Model is learning well.  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ What is an Optimizer?\n",
    "An optimizer updates the model weights to **minimize loss**.\n",
    "\n",
    "üìå **Adam Optimizer (`Adam`)**\n",
    "```python\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "```\n",
    "- Combines benefits of **SGD (Gradient Descent) + Momentum**.\n",
    "- Learns **faster** and adapts the **learning rate dynamically**.\n",
    "\n",
    "üîπ **Other Optimizers**  \n",
    "- **SGD** ‚Üí Slower but simpler.  \n",
    "- **RMSprop** ‚Üí Good for recurrent networks (**RNNs**).  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Training Step-by-Step\n",
    "\n",
    "1Ô∏è‚É£ `optimizer.zero_grad()` ‚Üí Clears old gradients.  \n",
    "2Ô∏è‚É£ `output = model(x_train)` ‚Üí Feeds input through the network.  \n",
    "3Ô∏è‚É£ `loss = criterion(output, y_train)` ‚Üí Calculates the error.  \n",
    "4Ô∏è‚É£ `loss.backward()` ‚Üí Computes gradients for each weight.  \n",
    "5Ô∏è‚É£ `optimizer.step()` ‚Üí Updates the weights based on gradients.  \n",
    "6Ô∏è‚É£ **Every 200 epochs**, prints the loss to track progress.  \n",
    "\n",
    "üìå **Why do we need 1000 epochs?**  \n",
    "- Small datasets need **more epochs** to generalize.  \n",
    "- Loss **decreases** over time as the model learns.  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Model Prediction Example\n",
    "After training, we pass `x = 10` into our model:\n",
    "\n",
    "```python\n",
    "y_pred = model(torch.tensor([10.0]))\n",
    "```\n",
    "üìå The model predicts **`y ‚âà 23`** (since it learned `y = 2x + 3`).  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
